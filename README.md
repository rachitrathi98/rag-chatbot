# ðŸ“š RAG Chatbot â€“ Local LLM (Ollama)

A **Retrieval-Augmented Generation (RAG) chatbot** that allows users to query PDF documents using a **fully local Large Language Model (LLM)** powered by **Ollama**.  
The application uses **FastAPI** for the backend, **Streamlit** for the frontend, and **LangChain** for RAG orchestration â€” with **no external APIs or cloud dependencies**.

---

## âœ¨ Features

- ðŸ“„ Query PDF documents
- âœ‚ï¸ Intelligent text chunking
- ðŸ§  Semantic search using vector embeddings
- ðŸ¤– Fully local LLM inference via Ollama
- ðŸ’¬ Conversational memory
- âš¡ FastAPI backend
- ðŸ–¥ï¸ Streamlit frontend
- ðŸ”’ Works fully offline

---


---

## ðŸ› ï¸ Tech Stack

- **Python 3.10+**
- **FastAPI**
- **Streamlit**
- **LangChain**
- **Ollama (Local LLM)**
- **ChromaDB / FAISS**
- **PyPDF**

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/rag-chatbot.git
## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Create a virtual environment

From the project root directory:

```bash
python -m venv .venv

cd rag-chatbot
python -m venv .venv
source .venv/bin/activate
.venv\Scripts\activate
pip install -r requirements.txt
Install Ollama

Open New Terminal for locally running Ollama

ollama pull llama3
ollama serve (http://localhost:11434)
source .venv/bin/activate
.venv\Scripts\activate
pip install -r requirements.txt

Open New Terminal for Running Backend
cd rag-chatbot
source .venv/bin/activate   # macOS / Linux
# OR
.venv\Scripts\activate      # Windows
cd Backend


python -m uvicorn main:app --reload;  (http://localhost:8000
)

Open New Terminal for Running Frontend

cd rag-chatbot
source .venv/bin/activate   # macOS / Linux
# OR
.venv\Scripts\activate      # Windows
cd Frontend

streamlit run Frontend/app.py (http://localhost:8501
)

For PDfs add in here --> data/pdfs/ they will be automatically indexed if not added from browser




